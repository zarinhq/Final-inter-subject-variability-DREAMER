{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab4d94e",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c955316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wfdb wget tqdm biosppy imbalanced-learn seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2be47",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb484014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import wget\n",
    "import math\n",
    "import zipfile\n",
    "import wfdb as wf\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import datetime\n",
    "import cv2\n",
    "import random\n",
    "import itertools\n",
    "import scipy.io\n",
    "import random as python_random\n",
    "import scipy.interpolate as interp\n",
    "from scipy import signal\n",
    "from scipy.signal import resample\n",
    "from scipy.signal import find_peaks\n",
    "from pathlib import Path\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.fft import fft, ifft, fftfreq, rfft,irfft, rfftfreq\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "   tf.random.set_seed(seed_value)\n",
    "   np.random.seed(seed_value)\n",
    "   random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aebb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed_value)\n",
    "vec = np.random.randint(1, 10)\n",
    "print(vec)\n",
    "random.seed(42)\n",
    "print(random.random())\n",
    "tf.random.set_seed(42)\n",
    "print(tf.random.uniform([1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82a250",
   "metadata": {},
   "source": [
    "## Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddfd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_subject_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22]\n",
    "test_subject_env_value_str = os.environ.get('TEST_SUBJECT_ARG')\n",
    "test_subject_env_value = int(test_subject_env_value_str)\n",
    "test_subject = [test_subject_env_value]\n",
    "rest_subjects = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22]\n",
    "rest_subjects.remove(test_subject_env_value)\n",
    "print(test_subject)\n",
    "print(rest_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d37d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH = 'DREAMER.mat'\n",
    "\n",
    "def load_data(subject_id): \n",
    "    basl_left = []\n",
    "    valence_left = []\n",
    "    arousal_left = []\n",
    "    \n",
    "    data_file = scipy.io.loadmat(SOURCE_PATH)\n",
    "    \n",
    "    valence_path = (data_file[\"DREAMER\"][0, 0][\"Data\"][0, subject_id][\"ScoreValence\"][0, 0])\n",
    "    arousal_path = (data_file[\"DREAMER\"][0, 0][\"Data\"][0, subject_id][\"ScoreArousal\"][0, 0])\n",
    "    \n",
    "    print('Loading data for S'+ str(subject_id))\n",
    "    \n",
    "    for video in range(0, 18):\n",
    "        basl_left.append(data_file[\"DREAMER\"][0, 0][\"Data\"]\n",
    "                    [0, subject_id][\"ECG\"][0, 0]\n",
    "                    [\"baseline\"][0, 0][video, 0][:, 0])\n",
    "        stim_left = (data_file[\"DREAMER\"][0, 0][\"Data\"]\n",
    "                         [0, subject_id][\"ECG\"][0, 0]\n",
    "                         [\"stimuli\"][0, 0][video, 0][:, 0])\n",
    "\n",
    "        if (valence_path[video, 0] > arousal_path[video, 0]):\n",
    "            valence_left.append(stim_left)\n",
    "        else:\n",
    "            if (valence_path[video, 0] < arousal_path[video, 0]):\n",
    "                arousal_left.append(stim_left)\n",
    "            \n",
    "    Full_basl_left = list(itertools.chain.from_iterable(basl_left))\n",
    "    Full_valence_left = list(itertools.chain.from_iterable(valence_left))\n",
    "    Full_arousal_left = list(itertools.chain.from_iterable(arousal_left))\n",
    "    \n",
    "            \n",
    "    return Full_basl_left, Full_valence_left, Full_arousal_left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee5790",
   "metadata": {},
   "source": [
    "##  Data Preprocesssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ef6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "debug = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 256.0\n",
    "lowcut = 4.0\n",
    "highcut = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBand(signal, highcut, lowcut, order, fs):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    s3 = lfilter(b, a, signal)\n",
    "    return s3\n",
    "    \n",
    "def cutBandHelper(signal, highcut, lowcut, order, fs):\n",
    "    new_signal = signal.copy()\n",
    "    nyq = 0.5 * fs\n",
    "    lowcut = lowcut/nyq\n",
    "    highcut = highcut/nyq\n",
    "    b, a = butter(order, lowcut, btype = 'low')\n",
    "    d, c = butter(order, highcut, btype = 'high')\n",
    "    s1 = lfilter(b, a, signal)\n",
    "    s2 = lfilter(d, c, new_signal)\n",
    "    return s1, s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f776674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(subject_ids):\n",
    "    all_subject_data = np.array([]);\n",
    "    all_subject_labels = np.array([]);\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        if(debug): print(\"Processing subject id: \", subject_id);\n",
    "        signal_data = load_data(subject_id)\n",
    "        \n",
    "        ecg_baseline = signal_data[0]\n",
    "        ecg_valence = signal_data[1]\n",
    "        ecg_arousal = signal_data[2]\n",
    "        \n",
    "        \n",
    "        if(debug):\n",
    "            print(\"Raw ecg_baseline:\", ecg_baseline)\n",
    "            print(\"Raw ecg_valence:\", ecg_valence)\n",
    "            print(\"Raw ecg_arousal:\", ecg_arousal)\n",
    "            \n",
    "        ecg_baseline_data = np.array(ecg_baseline)\n",
    "        ecg_valence_data = np.array(ecg_valence)\n",
    "        ecg_arousal_data = np.array(ecg_arousal)\n",
    "        \n",
    "        ecg_arousal_data = ecg_arousal_data[0:208128]\n",
    "\n",
    "        if(debug):\n",
    "            plt.plot(ecg_baseline_data)\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(ecg_arousal_data)\n",
    "            plt.show()\n",
    "            \n",
    "    return ecg_baseline_data, ecg_arousal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937877ef",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4027d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_subject_data = np.array([]);\n",
    "test_ecg_baseline_data, test_ecg_arousal_data = generateData(test_subject)\n",
    "\n",
    "test_baseline_filtered = getBand(test_ecg_baseline_data, highcut, lowcut, 1, fs)\n",
    "test_arousal_filtered = getBand(test_ecg_arousal_data, highcut, lowcut, 1, fs)\n",
    "\n",
    "window_size = 256\n",
    "window_shift = 256\n",
    "\n",
    "test_heart_beat_base = []\n",
    "for i in range(0,len(test_ecg_baseline_data) - window_size,window_shift):\n",
    "    test_heart_beat_base.append(test_ecg_baseline_data[i:window_size + i])\n",
    "test_heart_beat_base.pop()\n",
    "\n",
    "test_heart_beat_arousal = []\n",
    "for i in range(0,len(test_ecg_arousal_data) - window_size,window_shift):\n",
    "    test_heart_beat_arousal.append(test_ecg_arousal_data[i:window_size + i])\n",
    "test_heart_beat_arousal.pop()\n",
    "\n",
    "\n",
    "for idx, idxval in enumerate(test_heart_beat_base):\n",
    "    test_heart_beat_base[idx] = (test_heart_beat_base[idx] - test_heart_beat_base[idx].min()) / test_heart_beat_base[idx].ptp() # Normalize the readings to a 0-1 range \n",
    "    test_heart_beat_base[idx] = np.append(test_heart_beat_base[idx], 0.0) #baseline = 0\n",
    "\n",
    "for idx, idxval in enumerate(test_heart_beat_arousal):\n",
    "    test_heart_beat_arousal[idx] = (test_heart_beat_arousal[idx] - test_heart_beat_arousal[idx].min()) / test_heart_beat_arousal[idx].ptp() # Normalize the readings to a 0-1 range \n",
    "    test_heart_beat_arousal[idx] = np.append(test_heart_beat_arousal[idx], 1.0) #arousal = 1 \n",
    "\n",
    "test_heart_beat_all = np.concatenate((test_heart_beat_base, test_heart_beat_arousal), axis=0)\n",
    "test_subject_data = np.array(list(test_heart_beat_all[:]), dtype=float)\n",
    "\n",
    "if(test_all_subject_data.size == 0):\n",
    "    test_all_subject_data = test_subject_data\n",
    "else:\n",
    "    if(test_subject_data.size != 0):\n",
    "        test_all_subject_data = np.concatenate((test_all_subject_data, test_subject_data), axis=0)\n",
    "\n",
    "print(\"New data shape\", test_subject_data.shape, \"Total Shape: \", test_all_subject_data.shape)\n",
    "\n",
    "#SMOTE to balance the data\n",
    "test_df_final_data_X = pd.DataFrame(data=test_all_subject_data[:, :-1])\n",
    "test_df_final_data_Y = pd.DataFrame(data=test_all_subject_data[:,-1])\n",
    "\n",
    "test_smote = SMOTE(sampling_strategy='not majority')\n",
    "test_data, test_labels = test_smote.fit_resample(test_df_final_data_X, test_df_final_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ed1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test data shape:\", test_data.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n",
    "print(\"Dimension:\", test_labels.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.DataFrame(test_labels).to_numpy()\n",
    "y_test = test_labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab10069",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test labels shape:\", y_test.shape)\n",
    "print(\"Dimension:\", y_test.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ac691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape test data to (n_samples, 256, 1), where each sample is of size (256, 1)\n",
    "X_test = np.array(test_data).reshape(test_data.shape[0], test_data.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d74687",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of X_test:', X_test.shape)\n",
    "print('size of y_test:', y_test.shape)\n",
    "\n",
    "print('Test:', Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8a562",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238622b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subject_data = np.array([]);\n",
    "\n",
    "for idx, idxval in enumerate(rest_subjects):\n",
    "    ecg_baseline_data, ecg_arousal_data = generateData([idxval])\n",
    "    \n",
    "    baseline_s1, baseline_s2 = cutBandHelper(ecg_baseline_data, highcut, lowcut, 1, fs)\n",
    "    arousal_s1, arousal_s2 = cutBandHelper(ecg_arousal_data, highcut, lowcut, 1, fs)\n",
    "    \n",
    "    new_baseline = np.sum([baseline_s1, test_baseline_filtered, baseline_s2], axis=0)\n",
    "    new_arousal = np.sum([arousal_s1, test_arousal_filtered, arousal_s2], axis=0)\n",
    "    \n",
    "    print('reformed base size:', new_baseline.size)\n",
    "    print('reformed valence size:', new_arousal.size)\n",
    "    \n",
    "    window_size = 256\n",
    "    window_shift = 256\n",
    "\n",
    "    heart_beat_base = []\n",
    "    for i in range(0,len(new_baseline) - window_size,window_shift):\n",
    "        heart_beat_base.append(new_baseline[i:window_size + i])\n",
    "    heart_beat_base.pop()\n",
    "\n",
    "    heart_beat_arousal = []\n",
    "    for i in range(0,len(new_arousal) - window_size,window_shift):\n",
    "        heart_beat_arousal.append(new_arousal[i:window_size + i])\n",
    "    heart_beat_arousal.pop()\n",
    "\n",
    "\n",
    "    for idx, idxval in enumerate(heart_beat_base):\n",
    "        heart_beat_base[idx] = (heart_beat_base[idx] - heart_beat_base[idx].min()) / heart_beat_base[idx].ptp() # Normalize the readings to a 0-1 range \n",
    "        heart_beat_base[idx] = np.append(heart_beat_base[idx], 0.0) #baseline = 0\n",
    "\n",
    "    for idx, idxval in enumerate(heart_beat_arousal):\n",
    "        heart_beat_arousal[idx] = (heart_beat_arousal[idx] - heart_beat_arousal[idx].min()) / heart_beat_arousal[idx].ptp() # Normalize the readings to a 0-1 range \n",
    "        heart_beat_arousal[idx] = np.append(heart_beat_arousal[idx], 1.0) #arousal = 1 \n",
    "\n",
    "    heart_beat_all = np.concatenate((heart_beat_base, heart_beat_arousal), axis=0)\n",
    "    subject_data = np.array(list(heart_beat_all[:]), dtype=float)\n",
    "\n",
    "    if(all_subject_data.size == 0):\n",
    "        all_subject_data = subject_data\n",
    "    else:\n",
    "        if(subject_data.size != 0):\n",
    "            all_subject_data = np.concatenate((all_subject_data, subject_data), axis=0)\n",
    "\n",
    "    print(\"New data shape\", subject_data.shape, \"Total Shape: \", all_subject_data.shape)\n",
    "\n",
    "#SMOTE to balance the data\n",
    "df_final_data_X = pd.DataFrame(data=all_subject_data[:, :-1])\n",
    "df_final_data_Y = pd.DataFrame(data=all_subject_data[:,-1])\n",
    "\n",
    "smote = SMOTE(sampling_strategy='not majority')\n",
    "train_data, train_labels = smote.fit_resample(df_final_data_X, df_final_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd849a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Dimension:\", train_labels.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e79bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.DataFrame(train_labels).to_numpy()\n",
    "train_labels = train_labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c2c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Dimension:\", train_labels.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e738c1a",
   "metadata": {},
   "source": [
    "## Train and Validation data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape train and validation data to (n_samples, 256, 1), where each sample is of size (256, 1)\n",
    "X_train = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_val = np.array(X_val).reshape(X_val.shape[0], X_val.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc160ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of X_train:', X_train.shape)\n",
    "print('size of X_val:', X_val.shape)\n",
    "\n",
    "print('size of y_train:', y_train.shape)\n",
    "print('size of y_val:', y_val.shape)\n",
    "\n",
    "print('Train:', Counter(y_train), '\\nVal:', Counter(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c3315",
   "metadata": {},
   "source": [
    "## 1D CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9eae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, BatchNormalization, MaxPool1D\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random_seeds()\n",
    "# kernel_regularizer =tf.keras.regularizers.l2(l=0.004)\n",
    "# initializer = tf.keras.initializers.HeUniform()\n",
    "cnn_model = tf.keras.models.Sequential()\n",
    "cnn_model.add(Conv1D(filters=5, kernel_size=(5,), padding='same',  activation='relu', input_shape= (X_train.shape[1],1)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Conv1D(filters=10, kernel_size=(5,), padding='same', activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Conv1D(filters=15, kernel_size=(5,), padding='same', activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(units = 128, activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "cnn_model.add(Dense(units = 1, activation='sigmoid'))\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn_model.layers[0])\n",
    "print(cnn_model.layers[0].get_weights()[0])\n",
    "print(cnn_model.layers[0].get_weights()[0].shape)\n",
    "print(cnn_model.layers[0].get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = 0.001 * (np.random.rand(5, 1, 5))\n",
    "# print(weights)\n",
    "# bias = np.random.rand(5)\n",
    "# print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee835948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model.layers[0].set_weights([weights, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "  initial_lrate = 0.005\n",
    "  drop = 0.6\n",
    "  epochs_drop = 10.0\n",
    "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "  return lrate\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.0, momentum=0.6, nesterov=False)\n",
    "\n",
    "cnn_model.compile(optimizer= opt, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size, validation_data = (X_val, y_val), callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78134a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('1D CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "max_epoch = len(history.history['accuracy'])+1\n",
    "epoch_list = list(range(1,max_epoch))\n",
    "ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(1, max_epoch, 5))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca508f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f09f0",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_test_preds = np.around(cnn_model.predict(X_test))\n",
    "# print(y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedeef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27243ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_random_seeds()\n",
    "# # kernel_regularizer =tf.keras.regularizers.l2(l=0.004)\n",
    "# # initializer = tf.keras.initializers.HeUniform()\n",
    "# cnn_model = tf.keras.models.Sequential()\n",
    "# cnn_model.add(Conv1D(filters=5, kernel_size=(5,), padding='same',  activation='relu', input_shape= (X_train.shape[1],1)))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(Conv1D(filters=10, kernel_size=(5,), padding='same', activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(Conv1D(filters=15, kernel_size=(5,), padding='same', activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(Conv1D(filters=20, kernel_size=(5,), padding='same', activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(Flatten())\n",
    "# cnn_model.add(Dense(units = 128, activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "# cnn_model.add(Dense(units = 2, activation='softmax'))\n",
    "# cnn_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
