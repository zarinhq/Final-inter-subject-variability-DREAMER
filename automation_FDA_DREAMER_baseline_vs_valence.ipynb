{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab4d94e",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fcb787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c955316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wfdb in ./miniconda/lib/python3.9/site-packages (4.0.0)\n",
      "Requirement already satisfied: wget in ./miniconda/lib/python3.9/site-packages (3.2)\n",
      "Requirement already satisfied: tqdm in ./miniconda/lib/python3.9/site-packages (4.64.0)\n",
      "Requirement already satisfied: biosppy in ./miniconda/lib/python3.9/site-packages (1.0.0)\n",
      "Requirement already satisfied: imbalanced-learn in ./miniconda/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: seaborn in ./miniconda/lib/python3.9/site-packages (0.12.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.10.1 in ./miniconda/lib/python3.9/site-packages (from wfdb) (1.23.4)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.2.2 in ./miniconda/lib/python3.9/site-packages (from wfdb) (3.6.2)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in ./miniconda/lib/python3.9/site-packages (from wfdb) (1.5.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in ./miniconda/lib/python3.9/site-packages (from wfdb) (1.9.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.8.1 in ./miniconda/lib/python3.9/site-packages (from wfdb) (2.27.1)\n",
      "Requirement already satisfied: SoundFile<0.12.0,>=0.10.0 in ./miniconda/lib/python3.9/site-packages (from wfdb) (0.11.0)\n",
      "Requirement already satisfied: h5py in ./miniconda/lib/python3.9/site-packages (from biosppy) (3.6.0)\n",
      "Requirement already satisfied: scikit-learn in ./miniconda/lib/python3.9/site-packages (from biosppy) (1.1.3)\n",
      "Requirement already satisfied: six in ./miniconda/lib/python3.9/site-packages (from biosppy) (1.16.0)\n",
      "Requirement already satisfied: opencv-python in ./miniconda/lib/python3.9/site-packages (from biosppy) (4.6.0.66)\n",
      "Requirement already satisfied: bidict in ./miniconda/lib/python3.9/site-packages (from biosppy) (0.22.0)\n",
      "Requirement already satisfied: joblib in ./miniconda/lib/python3.9/site-packages (from biosppy) (1.2.0)\n",
      "Requirement already satisfied: shortuuid in ./miniconda/lib/python3.9/site-packages (from biosppy) (1.0.11)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./miniconda/lib/python3.9/site-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./miniconda/lib/python3.9/site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./miniconda/lib/python3.9/site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (1.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./miniconda/lib/python3.9/site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./miniconda/lib/python3.9/site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./miniconda/lib/python3.9/site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda/lib/python3.9/site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./miniconda/lib/python3.9/site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./miniconda/lib/python3.9/site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (9.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda/lib/python3.9/site-packages (from pandas<2.0.0,>=1.0.0->wfdb) (2022.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda/lib/python3.9/site-packages (from requests<3.0.0,>=2.8.1->wfdb) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda/lib/python3.9/site-packages (from requests<3.0.0,>=2.8.1->wfdb) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda/lib/python3.9/site-packages (from requests<3.0.0,>=2.8.1->wfdb) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./miniconda/lib/python3.9/site-packages (from requests<3.0.0,>=2.8.1->wfdb) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.0 in ./miniconda/lib/python3.9/site-packages (from SoundFile<0.12.0,>=0.10.0->wfdb) (1.15.0)\n",
      "Requirement already satisfied: pycparser in ./miniconda/lib/python3.9/site-packages (from cffi>=1.0->SoundFile<0.12.0,>=0.10.0->wfdb) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install wfdb wget tqdm biosppy imbalanced-learn seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2be47",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb484014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import wget\n",
    "import math\n",
    "import zipfile\n",
    "import wfdb as wf\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import datetime\n",
    "import cv2\n",
    "import random\n",
    "import itertools\n",
    "import scipy.io\n",
    "import random as python_random\n",
    "import scipy.interpolate as interp\n",
    "from scipy import signal\n",
    "from scipy.signal import resample\n",
    "from scipy.signal import find_peaks\n",
    "from pathlib import Path\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.fft import fft, ifft, fftfreq, rfft,irfft, rfftfreq\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353c7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "   tf.random.set_seed(seed_value)\n",
    "   np.random.seed(seed_value)\n",
    "   random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7aebb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0.6394267984578837\n",
      "Metal device set to: Apple M1\n",
      "tf.Tensor([0.6645621], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:40:20.758321: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-06-18 20:40:20.758411: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed_value)\n",
    "vec = np.random.randint(1, 10)\n",
    "print(vec)\n",
    "random.seed(42)\n",
    "print(random.random())\n",
    "tf.random.set_seed(42)\n",
    "print(tf.random.uniform([1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82a250",
   "metadata": {},
   "source": [
    "## Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ddfd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22]\n"
     ]
    }
   ],
   "source": [
    "# all_subject_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22]\n",
    "test_subject_env_value_str = os.environ.get('TEST_SUBJECT_ARG')\n",
    "test_subject_env_value = int(test_subject_env_value_str)\n",
    "test_subject = [test_subject_env_value]\n",
    "rest_subjects = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22]\n",
    "rest_subjects.remove(test_subject_env_value)\n",
    "print(test_subject)\n",
    "print(rest_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d37d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH = 'DREAMER.mat'\n",
    "\n",
    "def load_data(subject_id): \n",
    "    basl_left = []\n",
    "    valence_left = []\n",
    "    arousal_left = []\n",
    "    \n",
    "    data_file = scipy.io.loadmat(SOURCE_PATH)\n",
    "    \n",
    "    valence_path = (data_file[\"DREAMER\"][0, 0][\"Data\"][0, subject_id][\"ScoreValence\"][0, 0])\n",
    "    arousal_path = (data_file[\"DREAMER\"][0, 0][\"Data\"][0, subject_id][\"ScoreArousal\"][0, 0])\n",
    "    \n",
    "    print('Loading data for S'+ str(subject_id))\n",
    "    \n",
    "    for video in range(0, 18):\n",
    "        basl_left.append(data_file[\"DREAMER\"][0, 0][\"Data\"]\n",
    "                    [0, subject_id][\"ECG\"][0, 0]\n",
    "                    [\"baseline\"][0, 0][video, 0][:, 0])\n",
    "        stim_left = (data_file[\"DREAMER\"][0, 0][\"Data\"]\n",
    "                         [0, subject_id][\"ECG\"][0, 0]\n",
    "                         [\"stimuli\"][0, 0][video, 0][:, 0])\n",
    "\n",
    "        if (valence_path[video, 0] > arousal_path[video, 0]):\n",
    "            valence_left.append(stim_left)\n",
    "        else:\n",
    "            if (valence_path[video, 0] < arousal_path[video, 0]):\n",
    "                arousal_left.append(stim_left)\n",
    "            \n",
    "    Full_basl_left = list(itertools.chain.from_iterable(basl_left))\n",
    "    Full_valence_left = list(itertools.chain.from_iterable(valence_left))\n",
    "    Full_arousal_left = list(itertools.chain.from_iterable(arousal_left))\n",
    "    \n",
    "            \n",
    "    return Full_basl_left, Full_valence_left, Full_arousal_left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee5790",
   "metadata": {},
   "source": [
    "##  Data Preprocesssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50ef6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "debug = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eec723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 256.0\n",
    "lowcut = 4.0\n",
    "highcut = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d7584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBand(signal, highcut, lowcut, order, fs):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    s3 = lfilter(b, a, signal)\n",
    "    return s3\n",
    "    \n",
    "def cutBandHelper(signal, highcut, lowcut, order, fs):\n",
    "    new_signal = signal.copy()\n",
    "    nyq = 0.5 * fs\n",
    "    lowcut = lowcut/nyq\n",
    "    highcut = highcut/nyq\n",
    "    b, a = butter(order, lowcut, btype = 'low')\n",
    "    d, c = butter(order, highcut, btype = 'high')\n",
    "    s1 = lfilter(b, a, signal)\n",
    "    s2 = lfilter(d, c, new_signal)\n",
    "    return s1, s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f776674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(subject_ids):\n",
    "    all_subject_data = np.array([]);\n",
    "    all_subject_labels = np.array([]);\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        if(debug): print(\"Processing subject id: \", subject_id);\n",
    "        signal_data = load_data(subject_id)\n",
    "        \n",
    "        ecg_baseline = signal_data[0]\n",
    "        ecg_valence = signal_data[1]\n",
    "        ecg_arousal = signal_data[2]\n",
    "        \n",
    "        \n",
    "        if(debug):\n",
    "            print(\"Raw ecg_baseline:\", ecg_baseline)\n",
    "            print(\"Raw ecg_valence:\", ecg_valence)\n",
    "            print(\"Raw ecg_arousal:\", ecg_arousal)\n",
    "            \n",
    "        ecg_baseline_data = np.array(ecg_baseline)\n",
    "        ecg_valence_data = np.array(ecg_valence)\n",
    "        ecg_arousal_data = np.array(ecg_arousal)\n",
    "        \n",
    "        ecg_valence_data = ecg_valence_data[0:120064]\n",
    "\n",
    "        if(debug):\n",
    "            plt.plot(ecg_baseline_data)\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(ecg_valence_data)\n",
    "            plt.show()\n",
    "            \n",
    "    return ecg_baseline_data, ecg_valence_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937877ef",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4027d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for S9\n",
      "New data shape (1563, 257) Total Shape:  (1563, 257)\n"
     ]
    }
   ],
   "source": [
    "test_all_subject_data = np.array([]);\n",
    "test_ecg_baseline_data, test_ecg_valence_data = generateData(test_subject)\n",
    "\n",
    "test_baseline_filtered = getBand(test_ecg_baseline_data, highcut, lowcut, 1, fs)\n",
    "test_valence_filtered = getBand(test_ecg_valence_data, highcut, lowcut, 1, fs)\n",
    "\n",
    "window_size = 256\n",
    "window_shift = 256\n",
    "\n",
    "test_heart_beat_base = []\n",
    "for i in range(0,len(test_ecg_baseline_data) - window_size,window_shift):\n",
    "    test_heart_beat_base.append(test_ecg_baseline_data[i:window_size + i])\n",
    "test_heart_beat_base.pop()\n",
    "\n",
    "test_heart_beat_valence = []\n",
    "for i in range(0,len(test_ecg_valence_data) - window_size,window_shift):\n",
    "    test_heart_beat_valence.append(test_ecg_valence_data[i:window_size + i])\n",
    "test_heart_beat_valence.pop()\n",
    "\n",
    "\n",
    "for idx, idxval in enumerate(test_heart_beat_base):\n",
    "    test_heart_beat_base[idx] = (test_heart_beat_base[idx] - test_heart_beat_base[idx].min()) / test_heart_beat_base[idx].ptp() # Normalize the readings to a 0-1 range \n",
    "    test_heart_beat_base[idx] = np.append(test_heart_beat_base[idx], 0.0) #baseline = 0\n",
    "\n",
    "for idx, idxval in enumerate(test_heart_beat_valence):\n",
    "    test_heart_beat_valence[idx] = (test_heart_beat_valence[idx] - test_heart_beat_valence[idx].min()) / test_heart_beat_valence[idx].ptp() # Normalize the readings to a 0-1 range \n",
    "    test_heart_beat_valence[idx] = np.append(test_heart_beat_valence[idx], 1.0) #valence = 1 \n",
    "\n",
    "test_heart_beat_all = np.concatenate((test_heart_beat_base, test_heart_beat_valence), axis=0)\n",
    "test_subject_data = np.array(list(test_heart_beat_all[:]), dtype=float)\n",
    "\n",
    "if(test_all_subject_data.size == 0):\n",
    "    test_all_subject_data = test_subject_data\n",
    "else:\n",
    "    if(test_subject_data.size != 0):\n",
    "        test_all_subject_data = np.concatenate((test_all_subject_data, test_subject_data), axis=0)\n",
    "\n",
    "print(\"New data shape\", test_subject_data.shape, \"Total Shape: \", test_all_subject_data.shape)\n",
    "\n",
    "#SMOTE to balance the data\n",
    "test_df_final_data_X = pd.DataFrame(data=test_all_subject_data[:, :-1])\n",
    "test_df_final_data_Y = pd.DataFrame(data=test_all_subject_data[:,-1])\n",
    "\n",
    "test_smote = SMOTE(sampling_strategy='not majority')\n",
    "test_data, test_labels = test_smote.fit_resample(test_df_final_data_X, test_df_final_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca9ed1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1096\n",
       "1.0    1096\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5fb3ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (2192, 256)\n",
      "Test labels shape: (2192, 1)\n",
      "Dimension: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data shape:\", test_data.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n",
    "print(\"Dimension:\", test_labels.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b2b7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.DataFrame(test_labels).to_numpy()\n",
    "y_test = test_labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ab10069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test labels shape: (2192,)\n",
      "Dimension: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Test labels shape:\", y_test.shape)\n",
    "print(\"Dimension:\", y_test.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a53ac691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape test data to (n_samples, 256, 1), where each sample is of size (256, 1)\n",
    "X_test = np.array(test_data).reshape(test_data.shape[0], test_data.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44d74687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X_test: (2192, 256, 1)\n",
      "size of y_test: (2192,)\n",
      "Test: Counter({0.0: 1096, 1.0: 1096})\n"
     ]
    }
   ],
   "source": [
    "print('size of X_test:', X_test.shape)\n",
    "print('size of y_test:', y_test.shape)\n",
    "\n",
    "print('Test:', Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8a562",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "238622b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for S0\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (1563, 257)\n",
      "Loading data for S1\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (3126, 257)\n",
      "Loading data for S2\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (4689, 257)\n",
      "Loading data for S3\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (6252, 257)\n",
      "Loading data for S4\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (7815, 257)\n",
      "Loading data for S5\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (9378, 257)\n",
      "Loading data for S6\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (10941, 257)\n",
      "Loading data for S7\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (12504, 257)\n",
      "Loading data for S8\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (14067, 257)\n",
      "Loading data for S10\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (15630, 257)\n",
      "Loading data for S11\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (17193, 257)\n",
      "Loading data for S12\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (18756, 257)\n",
      "Loading data for S13\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (20319, 257)\n",
      "Loading data for S14\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (21882, 257)\n",
      "Loading data for S15\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (23445, 257)\n",
      "Loading data for S16\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (25008, 257)\n",
      "Loading data for S17\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (26571, 257)\n",
      "Loading data for S19\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (28134, 257)\n",
      "Loading data for S20\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (29697, 257)\n",
      "Loading data for S21\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (31260, 257)\n",
      "Loading data for S22\n",
      "reformed base size: 281088\n",
      "reformed valence size: 120064\n",
      "New data shape (1563, 257) Total Shape:  (32823, 257)\n"
     ]
    }
   ],
   "source": [
    "all_subject_data = np.array([]);\n",
    "\n",
    "for idx, idxval in enumerate(rest_subjects):\n",
    "    ecg_baseline_data, ecg_valence_data = generateData([idxval])\n",
    "    \n",
    "    baseline_s1, baseline_s2 = cutBandHelper(ecg_baseline_data, highcut, lowcut, 1, fs)\n",
    "    valence_s1, valence_s2 = cutBandHelper(ecg_valence_data, highcut, lowcut, 1, fs)\n",
    "    \n",
    "    new_baseline = np.sum([baseline_s1, test_baseline_filtered, baseline_s2], axis=0)\n",
    "    new_valence = np.sum([valence_s1, test_valence_filtered, valence_s2], axis=0)\n",
    "    \n",
    "    print('reformed base size:', new_baseline.size)\n",
    "    print('reformed valence size:', new_valence.size)\n",
    "    \n",
    "    window_size = 256\n",
    "    window_shift = 256\n",
    "\n",
    "    heart_beat_base = []\n",
    "    for i in range(0,len(new_baseline) - window_size,window_shift):\n",
    "        heart_beat_base.append(new_baseline[i:window_size + i])\n",
    "    heart_beat_base.pop()\n",
    "\n",
    "    heart_beat_valence = []\n",
    "    for i in range(0,len(new_valence) - window_size,window_shift):\n",
    "        heart_beat_valence.append(new_valence[i:window_size + i])\n",
    "    heart_beat_valence.pop()\n",
    "\n",
    "\n",
    "    for idx, idxval in enumerate(heart_beat_base):\n",
    "        heart_beat_base[idx] = (heart_beat_base[idx] - heart_beat_base[idx].min()) / heart_beat_base[idx].ptp() # Normalize the readings to a 0-1 range \n",
    "        heart_beat_base[idx] = np.append(heart_beat_base[idx], 0.0) #baseline = 0\n",
    "\n",
    "    for idx, idxval in enumerate(heart_beat_valence):\n",
    "        heart_beat_valence[idx] = (heart_beat_valence[idx] - heart_beat_valence[idx].min()) / heart_beat_valence[idx].ptp() # Normalize the readings to a 0-1 range \n",
    "        heart_beat_valence[idx] = np.append(heart_beat_valence[idx], 1.0) #valence = 1 \n",
    "\n",
    "    heart_beat_all = np.concatenate((heart_beat_base, heart_beat_valence), axis=0)\n",
    "    subject_data = np.array(list(heart_beat_all[:]), dtype=float)\n",
    "\n",
    "    if(all_subject_data.size == 0):\n",
    "        all_subject_data = subject_data\n",
    "    else:\n",
    "        if(subject_data.size != 0):\n",
    "            all_subject_data = np.concatenate((all_subject_data, subject_data), axis=0)\n",
    "\n",
    "    print(\"New data shape\", subject_data.shape, \"Total Shape: \", all_subject_data.shape)\n",
    "\n",
    "#SMOTE to balance the data\n",
    "df_final_data_X = pd.DataFrame(data=all_subject_data[:, :-1])\n",
    "df_final_data_Y = pd.DataFrame(data=all_subject_data[:,-1])\n",
    "\n",
    "smote = SMOTE(sampling_strategy='not majority')\n",
    "train_data, train_labels = smote.fit_resample(df_final_data_X, df_final_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4e9897d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    23016\n",
       "1.0    23016\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfd849a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (46032, 256)\n",
      "Train labels shape: (46032, 1)\n",
      "Dimension: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Dimension:\", train_labels.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6e79bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.DataFrame(train_labels).to_numpy()\n",
    "train_labels = train_labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8c2c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels shape: (46032,)\n",
      "Dimension: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Dimension:\", train_labels.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e738c1a",
   "metadata": {},
   "source": [
    "## Train and Validation data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "041b69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0d7c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape train and validation data to (n_samples, 256, 1), where each sample is of size (256, 1)\n",
    "X_train = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_val = np.array(X_val).reshape(X_val.shape[0], X_val.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fc160ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X_train: (32222, 256, 1)\n",
      "size of X_val: (13810, 256, 1)\n",
      "size of y_train: (32222,)\n",
      "size of y_val: (13810,)\n",
      "Train: Counter({0.0: 16111, 1.0: 16111}) \n",
      "Val: Counter({1.0: 6905, 0.0: 6905})\n"
     ]
    }
   ],
   "source": [
    "print('size of X_train:', X_train.shape)\n",
    "print('size of X_val:', X_val.shape)\n",
    "\n",
    "print('size of y_train:', y_train.shape)\n",
    "print('size of y_val:', y_val.shape)\n",
    "\n",
    "print('Train:', Counter(y_train), '\\nVal:', Counter(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c3315",
   "metadata": {},
   "source": [
    "## 1D CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df9eae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, BatchNormalization, MaxPool1D\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0203688b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 256, 5)            30        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256, 5)           20        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 128, 5)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128, 5)           20        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 128, 10)           260       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 128, 10)          40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 64, 10)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64, 10)           40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 64, 15)            765       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 64, 15)           60        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 32, 15)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 32, 15)           60        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 480)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               61568     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,992\n",
      "Trainable params: 62,872\n",
      "Non-trainable params: 120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reset_random_seeds()\n",
    "# kernel_regularizer =tf.keras.regularizers.l2(l=0.004)\n",
    "# initializer = tf.keras.initializers.HeUniform()\n",
    "cnn_model = tf.keras.models.Sequential()\n",
    "cnn_model.add(Conv1D(filters=5, kernel_size=(5,), padding='same',  activation='relu', input_shape= (X_train.shape[1],1)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Conv1D(filters=10, kernel_size=(5,), padding='same', activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Conv1D(filters=15, kernel_size=(5,), padding='same', activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(units = 128, activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "cnn_model.add(Dense(units = 1, activation='sigmoid'))\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5511067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.conv1d.Conv1D object at 0x17c43e370>\n",
      "[[[ 0.32037407  0.14545935 -0.12637293  0.40248287 -0.3773936 ]]\n",
      "\n",
      " [[ 0.13625014  0.42082787 -0.29702842  0.02127779  0.11695492]]\n",
      "\n",
      " [[-0.36991873 -0.04797933  0.41683322 -0.02652773  0.00263533]]\n",
      "\n",
      " [[ 0.23761892 -0.43037158 -0.27514288  0.01273805 -0.3878015 ]]\n",
      "\n",
      " [[-0.4135429   0.31123072  0.37157005 -0.21238995  0.3215751 ]]]\n",
      "(5, 1, 5)\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(cnn_model.layers[0])\n",
    "print(cnn_model.layers[0].get_weights()[0])\n",
    "print(cnn_model.layers[0].get_weights()[0].shape)\n",
    "print(cnn_model.layers[0].get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "060a3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = 0.001 * (np.random.rand(5, 1, 5))\n",
    "# print(weights)\n",
    "# bias = np.random.rand(5)\n",
    "# print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee835948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model.layers[0].set_weights([weights, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f259e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d28e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "  initial_lrate = 0.005\n",
    "  drop = 0.6\n",
    "  epochs_drop = 10.0\n",
    "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "  return lrate\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba4c7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.0, momentum=0.6, nesterov=False)\n",
    "\n",
    "cnn_model.compile(optimizer= opt, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c0b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:15:54.694849: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-06-18 20:15:55.419702: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-18 20:16:21.498464: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007/1007 - 33s - loss: 1.4766 - accuracy: 0.5842 - val_loss: 1.5034 - val_accuracy: 0.5524 - lr: 0.0050 - 33s/epoch - 32ms/step\n",
      "Epoch 2/50\n",
      "1007/1007 - 31s - loss: 1.2878 - accuracy: 0.6364 - val_loss: 1.6897 - val_accuracy: 0.5290 - lr: 0.0050 - 31s/epoch - 31ms/step\n",
      "Epoch 3/50\n",
      "1007/1007 - 31s - loss: 1.1507 - accuracy: 0.6561 - val_loss: 1.1149 - val_accuracy: 0.6416 - lr: 0.0050 - 31s/epoch - 31ms/step\n",
      "Epoch 4/50\n",
      "1007/1007 - 30s - loss: 1.0370 - accuracy: 0.6789 - val_loss: 1.0383 - val_accuracy: 0.6285 - lr: 0.0050 - 30s/epoch - 30ms/step\n",
      "Epoch 5/50\n",
      "1007/1007 - 30s - loss: 0.9443 - accuracy: 0.6922 - val_loss: 0.9289 - val_accuracy: 0.6810 - lr: 0.0050 - 30s/epoch - 30ms/step\n",
      "Epoch 6/50\n",
      "1007/1007 - 30s - loss: 0.8652 - accuracy: 0.7086 - val_loss: 0.8733 - val_accuracy: 0.6748 - lr: 0.0050 - 30s/epoch - 30ms/step\n",
      "Epoch 7/50\n",
      "1007/1007 - 31s - loss: 0.7979 - accuracy: 0.7245 - val_loss: 0.8106 - val_accuracy: 0.6914 - lr: 0.0050 - 31s/epoch - 30ms/step\n",
      "Epoch 8/50\n",
      "1007/1007 - 31s - loss: 0.7411 - accuracy: 0.7381 - val_loss: 0.7459 - val_accuracy: 0.7248 - lr: 0.0050 - 31s/epoch - 31ms/step\n",
      "Epoch 9/50\n",
      "1007/1007 - 31s - loss: 0.6901 - accuracy: 0.7557 - val_loss: 1.0155 - val_accuracy: 0.5957 - lr: 0.0050 - 31s/epoch - 31ms/step\n",
      "Epoch 10/50\n",
      "1007/1007 - 31s - loss: 0.6320 - accuracy: 0.7848 - val_loss: 0.6971 - val_accuracy: 0.7238 - lr: 0.0030 - 31s/epoch - 31ms/step\n",
      "Epoch 11/50\n",
      "1007/1007 - 31s - loss: 0.6018 - accuracy: 0.7955 - val_loss: 0.6804 - val_accuracy: 0.7385 - lr: 0.0030 - 31s/epoch - 31ms/step\n",
      "Epoch 12/50\n",
      "1007/1007 - 31s - loss: 0.5765 - accuracy: 0.8034 - val_loss: 0.6845 - val_accuracy: 0.7217 - lr: 0.0030 - 31s/epoch - 31ms/step\n",
      "Epoch 13/50\n",
      "1007/1007 - 31s - loss: 0.5537 - accuracy: 0.8109 - val_loss: 0.6764 - val_accuracy: 0.7259 - lr: 0.0030 - 31s/epoch - 31ms/step\n",
      "Epoch 14/50\n",
      "1007/1007 - 31s - loss: 0.5330 - accuracy: 0.8213 - val_loss: 0.6360 - val_accuracy: 0.7562 - lr: 0.0030 - 31s/epoch - 31ms/step\n",
      "Epoch 15/50\n",
      "1007/1007 - 31s - loss: 0.5153 - accuracy: 0.8275 - val_loss: 0.5895 - val_accuracy: 0.7734 - lr: 0.0030 - 31s/epoch - 31ms/step\n",
      "Epoch 16/50\n",
      "1007/1007 - 32s - loss: 0.4990 - accuracy: 0.8329 - val_loss: 0.6009 - val_accuracy: 0.7668 - lr: 0.0030 - 32s/epoch - 32ms/step\n",
      "Epoch 17/50\n",
      "1007/1007 - 32s - loss: 0.4820 - accuracy: 0.8401 - val_loss: 0.5787 - val_accuracy: 0.7781 - lr: 0.0030 - 32s/epoch - 31ms/step\n",
      "Epoch 18/50\n",
      "1007/1007 - 33s - loss: 0.4670 - accuracy: 0.8464 - val_loss: 0.5786 - val_accuracy: 0.7786 - lr: 0.0030 - 33s/epoch - 33ms/step\n",
      "Epoch 19/50\n",
      "1007/1007 - 32s - loss: 0.4547 - accuracy: 0.8520 - val_loss: 0.6351 - val_accuracy: 0.7361 - lr: 0.0030 - 32s/epoch - 32ms/step\n",
      "Epoch 20/50\n",
      "1007/1007 - 31s - loss: 0.4147 - accuracy: 0.8745 - val_loss: 0.5329 - val_accuracy: 0.8053 - lr: 0.0018 - 31s/epoch - 31ms/step\n",
      "Epoch 21/50\n",
      "1007/1007 - 31s - loss: 0.4026 - accuracy: 0.8809 - val_loss: 0.5357 - val_accuracy: 0.8008 - lr: 0.0018 - 31s/epoch - 30ms/step\n",
      "Epoch 22/50\n",
      "1007/1007 - 31s - loss: 0.3934 - accuracy: 0.8850 - val_loss: 0.5399 - val_accuracy: 0.7972 - lr: 0.0018 - 31s/epoch - 30ms/step\n",
      "Epoch 23/50\n",
      "1007/1007 - 31s - loss: 0.3843 - accuracy: 0.8907 - val_loss: 0.5670 - val_accuracy: 0.7846 - lr: 0.0018 - 31s/epoch - 31ms/step\n",
      "Epoch 24/50\n",
      "1007/1007 - 31s - loss: 0.3747 - accuracy: 0.8922 - val_loss: 0.5411 - val_accuracy: 0.7952 - lr: 0.0018 - 31s/epoch - 31ms/step\n",
      "Epoch 25/50\n",
      "1007/1007 - 31s - loss: 0.3690 - accuracy: 0.8950 - val_loss: 0.5207 - val_accuracy: 0.8081 - lr: 0.0018 - 31s/epoch - 30ms/step\n",
      "Epoch 26/50\n",
      "1007/1007 - 30s - loss: 0.3579 - accuracy: 0.9029 - val_loss: 0.5347 - val_accuracy: 0.8075 - lr: 0.0018 - 30s/epoch - 30ms/step\n",
      "Epoch 27/50\n",
      "1007/1007 - 31s - loss: 0.3519 - accuracy: 0.9028 - val_loss: 0.5391 - val_accuracy: 0.7990 - lr: 0.0018 - 31s/epoch - 30ms/step\n",
      "Epoch 28/50\n",
      "1007/1007 - 31s - loss: 0.3441 - accuracy: 0.9065 - val_loss: 0.5146 - val_accuracy: 0.8154 - lr: 0.0018 - 31s/epoch - 30ms/step\n",
      "Epoch 29/50\n",
      "1007/1007 - 31s - loss: 0.3354 - accuracy: 0.9110 - val_loss: 0.5124 - val_accuracy: 0.8126 - lr: 0.0018 - 31s/epoch - 30ms/step\n",
      "Epoch 30/50\n",
      "1007/1007 - 31s - loss: 0.3101 - accuracy: 0.9274 - val_loss: 0.5114 - val_accuracy: 0.8163 - lr: 0.0011 - 31s/epoch - 31ms/step\n",
      "Epoch 31/50\n",
      "1007/1007 - 30s - loss: 0.3025 - accuracy: 0.9302 - val_loss: 0.5130 - val_accuracy: 0.8145 - lr: 0.0011 - 30s/epoch - 30ms/step\n",
      "Epoch 32/50\n",
      "1007/1007 - 31s - loss: 0.2966 - accuracy: 0.9333 - val_loss: 0.5056 - val_accuracy: 0.8203 - lr: 0.0011 - 31s/epoch - 31ms/step\n",
      "Epoch 33/50\n",
      "1007/1007 - 33s - loss: 0.2922 - accuracy: 0.9339 - val_loss: 0.5283 - val_accuracy: 0.8123 - lr: 0.0011 - 33s/epoch - 33ms/step\n",
      "Epoch 34/50\n",
      "1007/1007 - 32s - loss: 0.2870 - accuracy: 0.9381 - val_loss: 0.5016 - val_accuracy: 0.8214 - lr: 0.0011 - 32s/epoch - 32ms/step\n",
      "Epoch 35/50\n",
      "1007/1007 - 32s - loss: 0.2822 - accuracy: 0.9392 - val_loss: 0.5063 - val_accuracy: 0.8245 - lr: 0.0011 - 32s/epoch - 32ms/step\n",
      "Epoch 36/50\n",
      "1007/1007 - 31s - loss: 0.2787 - accuracy: 0.9411 - val_loss: 0.5100 - val_accuracy: 0.8197 - lr: 0.0011 - 31s/epoch - 31ms/step\n",
      "Epoch 37/50\n",
      "1007/1007 - 34s - loss: 0.2734 - accuracy: 0.9425 - val_loss: 0.5032 - val_accuracy: 0.8252 - lr: 0.0011 - 34s/epoch - 34ms/step\n",
      "Epoch 38/50\n",
      "1007/1007 - 33s - loss: 0.2686 - accuracy: 0.9447 - val_loss: 0.5070 - val_accuracy: 0.8239 - lr: 0.0011 - 33s/epoch - 33ms/step\n",
      "Epoch 39/50\n",
      "1007/1007 - 32s - loss: 0.2654 - accuracy: 0.9461 - val_loss: 0.5032 - val_accuracy: 0.8239 - lr: 0.0011 - 32s/epoch - 32ms/step\n",
      "Epoch 40/50\n",
      "1007/1007 - 31s - loss: 0.2491 - accuracy: 0.9554 - val_loss: 0.4891 - val_accuracy: 0.8359 - lr: 6.4800e-04 - 31s/epoch - 31ms/step\n",
      "Epoch 41/50\n"
     ]
    }
   ],
   "source": [
    "history = cnn_model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size, validation_data = (X_val, y_val), callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78134a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('1D CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "max_epoch = len(history.history['accuracy'])+1\n",
    "epoch_list = list(range(1,max_epoch))\n",
    "ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(1, max_epoch, 5))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca508f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f09f0",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_test_preds = np.around(cnn_model.predict(X_test))\n",
    "# print(y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedeef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27243ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_random_seeds()\n",
    "# # kernel_regularizer =tf.keras.regularizers.l2(l=0.004)\n",
    "# # initializer = tf.keras.initializers.HeUniform()\n",
    "# cnn_model = tf.keras.models.Sequential()\n",
    "# cnn_model.add(Conv1D(filters=5, kernel_size=(5,), padding='same',  activation='relu', input_shape= (X_train.shape[1],1)))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(Conv1D(filters=10, kernel_size=(5,), padding='same', activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(Conv1D(filters=15, kernel_size=(5,), padding='same', activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(Conv1D(filters=20, kernel_size=(5,), padding='same', activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(MaxPool1D(pool_size=(2,), strides=2))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "# cnn_model.add(Flatten())\n",
    "# cnn_model.add(Dense(units = 128, activation='relu', kernel_regularizer =tf.keras.regularizers.l2(l=0.004)))\n",
    "# cnn_model.add(Dense(units = 2, activation='softmax'))\n",
    "# cnn_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
